


<------------------------7.1)------------------------>
->



package myDemoz;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;


public class myDemoz {

	public static class Map extends Mapper<LongWritable, Text, Text,Text>
	{
		
		
		public void map(LongWritable key,Text value,Context context)throws IOException ,InterruptedException 
		{
			String line = value.toString();
			String[] arr = line.split(",");
			
			if(arr[2].contains("Comedy")) {
				
				context.write(new Text("Comedy Movies"), new Text(line));
			}
			
		}
	}
	public static class Reduce extends Reducer<Text, Text, Text, Text>
	{   
		public void reduce(Text key,Iterable<Text> values ,Context context) throws  IOException ,InterruptedException
		{
			for(Text value:values) {
				context.write(key, value);
			}
		}
		
	}
	public static void main(String args[]) throws Exception {
		Configuration conf=new Configuration();
		Job job = Job.getInstance(conf, "Movies Details");/* crate a job */
		job.setJarByClass(myDemoz.class);
		job.setMapperClass(Map.class);
		job.setReducerClass(Reduce.class);
		
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(Text.class);
		
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(Text.class);
		
		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);
		
		Path outputPath=new Path(args[1]);
		FileInputFormat.addInputPath(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		
		outputPath.getFileSystem(conf).delete(outputPath, true);
		System.exit(job.waitForCompletion(true)?0:1);
		
		
		
		
		
		
		
	}
}





<------------------------7.2)------------------------>
->


package myDemoz;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;


public class myDemoz {

	public static class Map extends Mapper<LongWritable, Text, Text,Text>
	{
		
		
		public void map(LongWritable key,Text value,Context context)throws IOException ,InterruptedException 
		{
			String line = value.toString();
			String[] arr = line.split(",");
			
			if(arr[2].contains("Documentary") && arr[1].contains("(1995)")) {
				
				context.write(new Text("Documentary Movies 1995"), new Text(line));
			}
			
		}
	}
	public static class Reduce extends Reducer<Text, Text, Text, Text>
	{   
		public void reduce(Text key,Iterable<Text> values ,Context context) throws  IOException ,InterruptedException
		{
			for(Text value:values) {
				context.write(key, value);
			}
		}
		
	}
	public static void main(String args[]) throws Exception {
		Configuration conf=new Configuration();
		Job job = Job.getInstance(conf, "Movies Details");/* crate a job */
		job.setJarByClass(myDemoz.class);
		job.setMapperClass(Map.class);
		job.setReducerClass(Reduce.class);
		
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(Text.class);
		
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(Text.class);
		
		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);
		
		Path outputPath=new Path(args[1]);
		FileInputFormat.addInputPath(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		
		outputPath.getFileSystem(conf).delete(outputPath, true);
		System.exit(job.waitForCompletion(true)?0:1);
		
		
		
		
		
		
		
	}
}

<------------------------7.3)------------------------>
->
package myDemoz;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;


public class myDemoz {

	public static class Map extends Mapper<LongWritable, Text, Text,IntWritable>
	{
		public void map(LongWritable key,Text value,Context context)throws IOException ,InterruptedException 
		{
			String line = value.toString();
            String[] arr = line.split(","); // 

            if (arr[2].length()==0 || arr[2].isEmpty()) {
                context.write(new Text("Missing genre records"), new IntWritable(1));
            }
		}
	}
	public static class Reduce extends Reducer<Text, IntWritable, Text, IntWritable>
	{   
		public void reduce(Text key,Iterable<IntWritable> value ,Context context) throws  IOException ,InterruptedException
		{
			int sum=0;
			for (IntWritable i:value) {
				sum+=i.get();
				
			}
			context.write(key,new IntWritable(sum));
		}
		
	}
	public static void main(String args[]) throws Exception {
		Configuration conf=new Configuration();
		Job job = Job.getInstance(conf, "Movies Details");/* crate a job */
		job.setJarByClass(myDemoz.class);
		job.setMapperClass(Map.class);
		job.setReducerClass(Reduce.class);
		
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(IntWritable.class);
		
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		
		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);
		
		Path outputPath=new Path(args[1]);
		FileInputFormat.addInputPath(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		
		outputPath.getFileSystem(conf).delete(outputPath, true);
		System.exit(job.waitForCompletion(true)?0:1);
		
		
		
		
		
		
		
	}
}

<------------------------7.4)------------------------>
->

package myDemoz;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;


public class myDemoz {

	public static class Map extends Mapper<LongWritable, Text, Text,Text>
	{
		public void map(LongWritable key,Text value,Context context)throws IOException ,InterruptedException 
		{
			String line = value.toString();
            String[] arr = line.split(","); // 

            if (arr[1].contains("Gold")) {
                context.write(new Text(arr[1]), new Text(""));
            }
		}
	}
	public static class Reduce extends Reducer<Text, Text, Text, Text>
	{   
		public void reduce(Text key,Iterable<Text> value ,Context context) throws  IOException ,InterruptedException
		{
			
			context.write(key,new Text(""));
		}
		
	}
	public static void main(String args[]) throws Exception {
		Configuration conf=new Configuration();
		Job job = Job.getInstance(conf, "Movies Details");/* crate a job */
		job.setJarByClass(myDemoz.class);
		job.setMapperClass(Map.class);
		job.setReducerClass(Reduce.class);
		
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(Text.class);
		
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(Text.class);
		
		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);
		
		Path outputPath=new Path(args[1]);
		FileInputFormat.addInputPath(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		
		outputPath.getFileSystem(conf).delete(outputPath, true);
		System.exit(job.waitForCompletion(true)?0:1);
		
		
		
		
		
		
		
	}
}

<------------------------7.5)------------------------>
->

package myDemoz;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;


public class myDemoz {

	public static class Map extends Mapper<LongWritable, Text, Text,IntWritable>
	{
		public void map(LongWritable key,Text value,Context context)throws IOException ,InterruptedException 
		{
			String line = value.toString();
            String[] arr = line.split(",");
            
            String genres = arr[2];

            if (genres.contains("Drama") || genres.contains("Romantic")) {
                context.write(new Text("Drama and Romance Movies Count"), new IntWritable(1));
            }
		}
	}
	public static class Reduce extends Reducer<Text, IntWritable, Text, IntWritable>
	{   
		public void reduce(Text key,Iterable<IntWritable> value ,Context context) throws  IOException ,InterruptedException
		{
			
			int sum=0;
			for(IntWritable var:value) {
				sum=sum+var.get();
			}
			context.write(key, new IntWritable(sum));
		}
		
	}
	public static void main(String args[]) throws Exception {
		Configuration conf=new Configuration();
		Job job = Job.getInstance(conf, "Movies Details");/* crate a job */
		job.setJarByClass(myDemoz.class);
		job.setMapperClass(Map.class);
		job.setReducerClass(Reduce.class);
		
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(IntWritable.class);
		
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		
		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);
		
		Path outputPath=new Path(args[1]);
		FileInputFormat.addInputPath(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		
		outputPath.getFileSystem(conf).delete(outputPath, true);
		System.exit(job.waitForCompletion(true)?0:1);
		
		
		
		
		
		
		
	}
}
